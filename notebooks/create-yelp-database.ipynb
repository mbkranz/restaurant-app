{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates a Yelp POSTGRESSQL database\n",
    " - Uses the open source Yelp dataset and inserts json files into SQL tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents \n",
    "- importing json into python using pandas tools\n",
    "- importing json to postgresql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "from pandas.io.json._normalize import json_normalize,nested_to_record #flattening json in pandas df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_data_path = '/Users/michaelkranz/Documents/restaurant-app/data/yelp_dataset/'\n",
    "\n",
    "yelp_json_filenames = {\"tips\":'yelp_academic_dataset_tip.json',\"reviews\":'yelp_academic_dataset_review.json',\n",
    "\"business_info\":'yelp_academic_dataset_business.json',\"user\":'yelp_academic_dataset_user.json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "with open(yelp_data_path+yelp_json_filenames['business_info']) as json_file:\n",
    "    json_list = json_file.readlines()\n",
    "json_data = [json.loads(json_line) for json_line in json_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>business_id</th>\n      <th>name</th>\n      <th>address</th>\n      <th>city</th>\n      <th>state</th>\n      <th>postal_code</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>stars</th>\n      <th>review_count</th>\n      <th>is_open</th>\n      <th>attributes</th>\n      <th>categories</th>\n      <th>hours</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f9NumwFMBDn751xgFiRbNA</td>\n      <td>The Range At Lake Norman</td>\n      <td>10913 Bailey Rd</td>\n      <td>Cornelius</td>\n      <td>NC</td>\n      <td>28031</td>\n      <td>35.462724</td>\n      <td>-80.852612</td>\n      <td>3.5</td>\n      <td>36</td>\n      <td>1</td>\n      <td>{'BusinessAcceptsCreditCards': 'True', 'BikePa...</td>\n      <td>Active Life, Gun/Rifle Ranges, Guns &amp; Ammo, Sh...</td>\n      <td>{'Monday': '10:0-18:0', 'Tuesday': '11:0-20:0'...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Yzvjg0SayhoZgCljUJRF9Q</td>\n      <td>Carlos Santo, NMD</td>\n      <td>8880 E Via Linda, Ste 107</td>\n      <td>Scottsdale</td>\n      <td>AZ</td>\n      <td>85258</td>\n      <td>33.569404</td>\n      <td>-111.890264</td>\n      <td>5.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>{'GoodForKids': 'True', 'ByAppointmentOnly': '...</td>\n      <td>Health &amp; Medical, Fitness &amp; Instruction, Yoga,...</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "              business_id                      name  \\\n0  f9NumwFMBDn751xgFiRbNA  The Range At Lake Norman   \n1  Yzvjg0SayhoZgCljUJRF9Q         Carlos Santo, NMD   \n\n                     address        city state postal_code   latitude  \\\n0            10913 Bailey Rd   Cornelius    NC       28031  35.462724   \n1  8880 E Via Linda, Ste 107  Scottsdale    AZ       85258  33.569404   \n\n    longitude  stars  review_count  is_open  \\\n0  -80.852612    3.5            36        1   \n1 -111.890264    5.0             4        1   \n\n                                          attributes  \\\n0  {'BusinessAcceptsCreditCards': 'True', 'BikePa...   \n1  {'GoodForKids': 'True', 'ByAppointmentOnly': '...   \n\n                                          categories  \\\n0  Active Life, Gun/Rifle Ranges, Guns & Ammo, Sh...   \n1  Health & Medical, Fitness & Instruction, Yoga,...   \n\n                                               hours  \n0  {'Monday': '10:0-18:0', 'Tuesday': '11:0-20:0'...  \n1                                               None  "
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(json_data).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 nested json objects : attributes and hours\n",
    "#df_flattened_data = json_normalize(json_data,sep=\"_\")\n",
    "json_flattened_data = nested_to_record(json_data,sep=\"_\",) #dont need to convert to df as destination is SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'business_id': 'f9NumwFMBDn751xgFiRbNA',\n 'name': 'The Range At Lake Norman',\n 'address': '10913 Bailey Rd',\n 'city': 'Cornelius',\n 'state': 'NC',\n 'postal_code': '28031',\n 'latitude': 35.4627242,\n 'longitude': -80.8526119,\n 'stars': 3.5,\n 'review_count': 36,\n 'is_open': 1,\n 'categories': 'Active Life, Gun/Rifle Ranges, Guns & Ammo, Shopping',\n 'attributes_BusinessAcceptsCreditCards': 'True',\n 'attributes_BikeParking': 'True',\n 'attributes_GoodForKids': 'False',\n 'attributes_BusinessParking': \"{'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}\",\n 'attributes_ByAppointmentOnly': 'False',\n 'attributes_RestaurantsPriceRange2': '3',\n 'hours_Monday': '10:0-18:0',\n 'hours_Tuesday': '11:0-20:0',\n 'hours_Wednesday': '10:0-18:0',\n 'hours_Thursday': '11:0-20:0',\n 'hours_Friday': '11:0-20:0',\n 'hours_Saturday': '11:0-20:0',\n 'hours_Sunday': '13:0-18:0'}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for flattening in future, may want to expand to convert nested json string to json\n",
    "##see attributes_Business\n",
    "json_flattened_data[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing JSON in PostgreSQL\n",
    "- upon further research, it looks like we can store JSON directly in PostgreSQL without the traditional field format\n",
    "    - [Storing JSON in PostgreSQL: A must-know feature] (https://www.blendo.co/blog/storing-json-in-postgresql/)\n",
    "    - [Replacing EAV with JSONB in PostgreSQL*](https://coussej.github.io/2016/01/14/Replacing-EAV-with-JSONB-in-PostgreSQL/)\n",
    "\n",
    "    *EAV = Entity,Attribute,Value (ie three tables connected with joins to get fields for the entity)\n",
    "\n",
    "\n",
    "- find connection information \n",
    "\n",
    ">michaelkranz$ `psql`\n",
    "\n",
    ">michaelkranz=# `CREATE DATABASE restaurants`\n",
    "\n",
    ">michaelkranz=# `\\c restaurants`\n",
    "\n",
    ">restaurants=# `\\conninfo`\n",
    "\n",
    "> You are now connected to database \"restaurants\" as user \"michaelkranz\".\n",
    "restaurants=# \\conninfo\n",
    "You are connected to database \"restaurants\" as user \"michaelkranz\" via socket in \"/tmp\" at port \"5432\".\n",
    "\n",
    "- [CHAR and VARCHAR : no performance differences and character limits used to check and will return error if longer](https://www.postgresqltutorial.com/postgresql-char-varchar-text/)\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL \n",
    "import psycopg2 #postgressql driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_db_params = {'drivername': 'postgres',\n",
    "                'database':'restaurants',\n",
    "               'username': 'michaelkranz',\n",
    "               'password': 'helloworld',\n",
    "               'host': 'localhost',\n",
    "               'port': 5432}\n",
    "\n",
    "postgres_db_url = URL(**postgres_db_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(postgres_db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c46089c14885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json_data' is not defined"
     ]
    }
   ],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['business_id', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'attributes', 'categories', 'hours'])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.json_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_sql_str = '''\n",
    "CREATE TABLE AS (\n",
    "    business_id \n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Help on function create_engine in module sqlalchemy.engine:\n\ncreate_engine(*args, **kwargs)\n    Create a new :class:`_engine.Engine` instance.\n    \n    The standard calling form is to send the URL as the\n    first positional argument, usually a string\n    that indicates database dialect and connection arguments::\n    \n    \n        engine = create_engine(\"postgresql://scott:tiger@localhost/test\")\n    \n    Additional keyword arguments may then follow it which\n    establish various options on the resulting :class:`_engine.Engine`\n    and its underlying :class:`.Dialect` and :class:`_pool.Pool`\n    constructs::\n    \n        engine = create_engine(\"mysql://scott:tiger@hostname/dbname\",\n                                    encoding='latin1', echo=True)\n    \n    The string form of the URL is\n    ``dialect[+driver]://user:password@host/dbname[?key=value..]``, where\n    ``dialect`` is a database name such as ``mysql``, ``oracle``,\n    ``postgresql``, etc., and ``driver`` the name of a DBAPI, such as\n    ``psycopg2``, ``pyodbc``, ``cx_oracle``, etc.  Alternatively,\n    the URL can be an instance of :class:`~sqlalchemy.engine.url.URL`.\n    \n    ``**kwargs`` takes a wide variety of options which are routed\n    towards their appropriate components.  Arguments may be specific to\n    the :class:`_engine.Engine`, the underlying :class:`.Dialect`,\n    as well as the\n    :class:`_pool.Pool`.  Specific dialects also accept keyword arguments that\n    are unique to that dialect.   Here, we describe the parameters\n    that are common to most :func:`_sa.create_engine()` usage.\n    \n    Once established, the newly resulting :class:`_engine.Engine` will\n    request a connection from the underlying :class:`_pool.Pool` once\n    :meth:`_engine.Engine.connect` is called, or a method which depends on it\n    such as :meth:`_engine.Engine.execute` is invoked.   The\n    :class:`_pool.Pool` in turn\n    will establish the first actual DBAPI connection when this request\n    is received.   The :func:`_sa.create_engine` call itself does **not**\n    establish any actual DBAPI connections directly.\n    \n    .. seealso::\n    \n        :doc:`/core/engines`\n    \n        :doc:`/dialects/index`\n    \n        :ref:`connections_toplevel`\n    \n    :param case_sensitive=True: if False, result column names\n       will match in a case-insensitive fashion, that is,\n       ``row['SomeColumn']``.\n    \n    :param connect_args: a dictionary of options which will be\n        passed directly to the DBAPI's ``connect()`` method as\n        additional keyword arguments.  See the example\n        at :ref:`custom_dbapi_args`.\n    \n    :param convert_unicode=False: if set to True, causes\n        all :class:`.String` datatypes to act as though the\n        :paramref:`.String.convert_unicode` flag has been set to ``True``,\n        regardless of a setting of ``False`` on an individual :class:`.String`\n        type.  This has the effect of causing all :class:`.String` -based\n        columns to accommodate Python Unicode objects directly as though the\n        datatype were the :class:`.Unicode` type.\n    \n        .. deprecated:: 1.3\n    \n            The :paramref:`_sa.create_engine.convert_unicode` parameter\n            is deprecated and will be removed in a future release.\n            All modern DBAPIs now support Python Unicode directly and this\n            parameter is unnecessary.\n    \n    :param creator: a callable which returns a DBAPI connection.\n        This creation function will be passed to the underlying\n        connection pool and will be used to create all new database\n        connections. Usage of this function causes connection\n        parameters specified in the URL argument to be bypassed.\n    \n        This hook is not as flexible as the newer\n        :class:`_events.DialectEvents.do_connect` hook which allows complete\n        control over how a connection is made to the database, given the full\n        set of URL arguments and state beforehand.\n    \n        .. seealso::\n    \n            :class:`_events.DialectEvents.do_connect` - event hook that allows\n            full control over DBAPI connection mechanics.\n    \n            :ref:`custom_dbapi_args`\n    \n    :param echo=False: if True, the Engine will log all statements\n        as well as a ``repr()`` of their parameter lists to the default log\n        handler, which defaults to ``sys.stdout`` for output.   If set to the\n        string ``\"debug\"``, result rows will be printed to the standard output\n        as well. The ``echo`` attribute of ``Engine`` can be modified at any\n        time to turn logging on and off; direct control of logging is also\n        available using the standard Python ``logging`` module.\n    \n        .. seealso::\n    \n            :ref:`dbengine_logging` - further detail on how to configure\n            logging.\n    \n    :param echo_pool=False: if True, the connection pool will log\n        informational output such as when connections are invalidated\n        as well as when connections are recycled to the default log handler,\n        which defaults to ``sys.stdout`` for output.   If set to the string\n        ``\"debug\"``, the logging will include pool checkouts and checkins.\n        Direct control of logging is also available using the standard Python\n        ``logging`` module.\n    \n        .. seealso::\n    \n            :ref:`dbengine_logging` - further detail on how to configure\n            logging.\n    \n    \n    :param empty_in_strategy:  The SQL compilation strategy to use when\n        rendering an IN or NOT IN expression for :meth:`.ColumnOperators.in_`\n        where the right-hand side\n        is an empty set.   This is a string value that may be one of\n        ``static``, ``dynamic``, or ``dynamic_warn``.   The ``static``\n        strategy is the default, and an IN comparison to an empty set\n        will generate a simple false expression \"1 != 1\".   The ``dynamic``\n        strategy behaves like that of SQLAlchemy 1.1 and earlier, emitting\n        a false expression of the form \"expr != expr\", which has the effect\n        of evaluting to NULL in the case of a null expression.\n        ``dynamic_warn`` is the same as ``dynamic``, however also emits a\n        warning when an empty set is encountered; this because the \"dynamic\"\n        comparison is typically poorly performing on most databases.\n    \n        .. versionadded:: 1.2  Added the ``empty_in_strategy`` setting and\n           additionally defaulted the behavior for empty-set IN comparisons\n           to a static boolean expression.\n    \n    :param encoding: Defaults to ``utf-8``.  This is the string\n        encoding used by SQLAlchemy for string encode/decode\n        operations which occur within SQLAlchemy, **outside of\n        the DBAPIs own encoding facilities.**\n    \n        .. note:: The ``encoding`` parameter deals only with in-Python\n           encoding issues that were prevalent with many DBAPIs under  Python\n           2.  Under Python 3 it is mostly unused.   For  DBAPIs that require\n           client encoding configurations, such as those of MySQL and Oracle,\n           please consult specific :ref:`dialect documentation\n           <dialect_toplevel>` for details.\n    \n        All modern DBAPIs that work in Python 3 necessarily feature direct\n        support for Python unicode strings.   Under Python 2, this was not\n        always the case.  For those scenarios where the DBAPI is detected as\n        not supporting a Python ``unicode`` object under Python 2, this\n        encoding is used to determine the source/destination encoding.  It is\n        **not used** for those cases where the DBAPI handles unicode directly.\n    \n        To properly configure a system to accommodate Python ``unicode``\n        objects, the DBAPI should be configured to handle unicode to the\n        greatest degree as is appropriate - see the notes on unicode pertaining\n        to the specific target database in use at :ref:`dialect_toplevel`.\n    \n        Areas where string encoding may need to be accommodated\n        outside of the DBAPI, nearly always under **Python 2 only**,\n        include zero or more of:\n    \n        * the values passed to bound parameters, corresponding to\n          the :class:`.Unicode` type or the :class:`.String` type\n          when ``convert_unicode`` is ``True``;\n        * the values returned in result set columns corresponding\n          to the :class:`.Unicode` type or the :class:`.String`\n          type when ``convert_unicode`` is ``True``;\n        * the string SQL statement passed to the DBAPI's\n          ``cursor.execute()`` method;\n        * the string names of the keys in the bound parameter\n          dictionary passed to the DBAPI's ``cursor.execute()``\n          as well as ``cursor.setinputsizes()`` methods;\n        * the string column names retrieved from the DBAPI's\n          ``cursor.description`` attribute.\n    \n        When using Python 3, the DBAPI is required to support all of the above\n        values as Python ``unicode`` objects, which in Python 3 are just known\n        as ``str``.  In Python 2, the DBAPI does not specify unicode behavior\n        at all, so SQLAlchemy must make decisions for each of the above values\n        on a per-DBAPI basis - implementations are completely inconsistent in\n        their behavior.\n    \n    :param execution_options: Dictionary execution options which will\n        be applied to all connections.  See\n        :meth:`~sqlalchemy.engine.Connection.execution_options`\n    \n    :param hide_parameters: Boolean, when set to True, SQL statement parameters\n        will not be displayed in INFO logging nor will they be formatted into\n        the string representation of :class:`.StatementError` objects.\n    \n        .. versionadded:: 1.3.8\n    \n    :param implicit_returning=True: When ``True``, a RETURNING-\n        compatible construct, if available, will be used to\n        fetch newly generated primary key values when a single row\n        INSERT statement is emitted with no existing returning()\n        clause.  This applies to those backends which support RETURNING\n        or a compatible construct, including PostgreSQL, Firebird, Oracle,\n        Microsoft SQL Server.   Set this to ``False`` to disable\n        the automatic usage of RETURNING.\n    \n    :param isolation_level: this string parameter is interpreted by various\n        dialects in order to affect the transaction isolation level of the\n        database connection.   The parameter essentially accepts some subset of\n        these string arguments: ``\"SERIALIZABLE\"``, ``\"REPEATABLE READ\"``,\n        ``\"READ COMMITTED\"``, ``\"READ UNCOMMITTED\"`` and ``\"AUTOCOMMIT\"``.\n        Behavior here varies per backend, and\n        individual dialects should be consulted directly.\n    \n        Note that the isolation level can also be set on a\n        per-:class:`_engine.Connection` basis as well, using the\n        :paramref:`.Connection.execution_options.isolation_level`\n        feature.\n    \n        .. seealso::\n    \n            :attr:`_engine.Connection.default_isolation_level`\n            - view default level\n    \n            :paramref:`.Connection.execution_options.isolation_level`\n            - set per :class:`_engine.Connection` isolation level\n    \n            :ref:`SQLite Transaction Isolation <sqlite_isolation_level>`\n    \n            :ref:`PostgreSQL Transaction Isolation <postgresql_isolation_level>`\n    \n            :ref:`MySQL Transaction Isolation <mysql_isolation_level>`\n    \n            :ref:`session_transaction_isolation` - for the ORM\n    \n    :param json_deserializer: for dialects that support the\n        :class:`_types.JSON`\n        datatype, this is a Python callable that will convert a JSON string\n        to a Python object.  By default, the Python ``json.loads`` function is\n        used.\n    \n        .. versionchanged:: 1.3.7  The SQLite dialect renamed this from\n           ``_json_deserializer``.\n    \n    :param json_serializer: for dialects that support the :class:`_types.JSON`\n        datatype, this is a Python callable that will render a given object\n        as JSON.   By default, the Python ``json.dumps`` function is used.\n    \n        .. versionchanged:: 1.3.7  The SQLite dialect renamed this from\n           ``_json_serializer``.\n    \n    :param label_length=None: optional integer value which limits\n        the size of dynamically generated column labels to that many\n        characters. If less than 6, labels are generated as\n        \"_(counter)\". If ``None``, the value of\n        ``dialect.max_identifier_length``, which may be affected via the\n        :paramref:`_sa.create_engine.max_identifier_length` parameter,\n        is used instead.   The value of\n        :paramref:`_sa.create_engine.label_length`\n        may not be larger than that of\n        :paramref:`_sa.create_engine.max_identfier_length`.\n    \n        .. seealso::\n    \n            :paramref:`_sa.create_engine.max_identifier_length`\n    \n    :param listeners: A list of one or more\n        :class:`~sqlalchemy.interfaces.PoolListener` objects which will\n        receive connection pool events.\n    \n    :param logging_name:  String identifier which will be used within\n        the \"name\" field of logging records generated within the\n        \"sqlalchemy.engine\" logger. Defaults to a hexstring of the\n        object's id.\n    \n    :param max_identifier_length: integer; override the max_identifier_length\n        determined by the dialect.  if ``None`` or zero, has no effect.  This\n        is the database's configured maximum number of characters that may be\n        used in a SQL identifier such as a table name, column name, or label\n        name. All dialects determine this value automatically, however in the\n        case of a new database version for which this value has changed but\n        SQLAlchemy's dialect has not been adjusted, the value may be passed\n        here.\n    \n        .. versionadded:: 1.3.9\n    \n        .. seealso::\n    \n            :paramref:`_sa.create_engine.label_length`\n    \n    :param max_overflow=10: the number of connections to allow in\n        connection pool \"overflow\", that is connections that can be\n        opened above and beyond the pool_size setting, which defaults\n        to five. this is only used with :class:`~sqlalchemy.pool.QueuePool`.\n    \n    :param module=None: reference to a Python module object (the module\n        itself, not its string name).  Specifies an alternate DBAPI module to\n        be used by the engine's dialect.  Each sub-dialect references a\n        specific DBAPI which will be imported before first connect.  This\n        parameter causes the import to be bypassed, and the given module to\n        be used instead. Can be used for testing of DBAPIs as well as to\n        inject \"mock\" DBAPI implementations into the :class:`_engine.Engine`.\n    \n    :param paramstyle=None: The `paramstyle <http://legacy.python.org/dev/peps/pep-0249/#paramstyle>`_\n        to use when rendering bound parameters.  This style defaults to the\n        one recommended by the DBAPI itself, which is retrieved from the\n        ``.paramstyle`` attribute of the DBAPI.  However, most DBAPIs accept\n        more than one paramstyle, and in particular it may be desirable\n        to change a \"named\" paramstyle into a \"positional\" one, or vice versa.\n        When this attribute is passed, it should be one of the values\n        ``\"qmark\"``, ``\"numeric\"``, ``\"named\"``, ``\"format\"`` or\n        ``\"pyformat\"``, and should correspond to a parameter style known\n        to be supported by the DBAPI in use.\n    \n    :param pool=None: an already-constructed instance of\n        :class:`~sqlalchemy.pool.Pool`, such as a\n        :class:`~sqlalchemy.pool.QueuePool` instance. If non-None, this\n        pool will be used directly as the underlying connection pool\n        for the engine, bypassing whatever connection parameters are\n        present in the URL argument. For information on constructing\n        connection pools manually, see :ref:`pooling_toplevel`.\n    \n    :param poolclass=None: a :class:`~sqlalchemy.pool.Pool`\n        subclass, which will be used to create a connection pool\n        instance using the connection parameters given in the URL. Note\n        this differs from ``pool`` in that you don't actually\n        instantiate the pool in this case, you just indicate what type\n        of pool to be used.\n    \n    :param pool_logging_name:  String identifier which will be used within\n       the \"name\" field of logging records generated within the\n       \"sqlalchemy.pool\" logger. Defaults to a hexstring of the object's\n       id.\n    \n    :param pool_pre_ping: boolean, if True will enable the connection pool\n        \"pre-ping\" feature that tests connections for liveness upon\n        each checkout.\n    \n        .. versionadded:: 1.2\n    \n        .. seealso::\n    \n            :ref:`pool_disconnects_pessimistic`\n    \n    :param pool_size=5: the number of connections to keep open\n        inside the connection pool. This used with\n        :class:`~sqlalchemy.pool.QueuePool` as\n        well as :class:`~sqlalchemy.pool.SingletonThreadPool`.  With\n        :class:`~sqlalchemy.pool.QueuePool`, a ``pool_size`` setting\n        of 0 indicates no limit; to disable pooling, set ``poolclass`` to\n        :class:`~sqlalchemy.pool.NullPool` instead.\n    \n    :param pool_recycle=-1: this setting causes the pool to recycle\n        connections after the given number of seconds has passed. It\n        defaults to -1, or no timeout. For example, setting to 3600\n        means connections will be recycled after one hour. Note that\n        MySQL in particular will disconnect automatically if no\n        activity is detected on a connection for eight hours (although\n        this is configurable with the MySQLDB connection itself and the\n        server configuration as well).\n    \n        .. seealso::\n    \n            :ref:`pool_setting_recycle`\n    \n    :param pool_reset_on_return='rollback': set the\n        :paramref:`_pool.Pool.reset_on_return` parameter of the underlying\n        :class:`_pool.Pool` object, which can be set to the values\n        ``\"rollback\"``, ``\"commit\"``, or ``None``.\n    \n        .. seealso::\n    \n            :paramref:`_pool.Pool.reset_on_return`\n    \n    :param pool_timeout=30: number of seconds to wait before giving\n        up on getting a connection from the pool. This is only used\n        with :class:`~sqlalchemy.pool.QueuePool`.\n    \n    :param pool_use_lifo=False: use LIFO (last-in-first-out) when retrieving\n        connections from :class:`.QueuePool` instead of FIFO\n        (first-in-first-out). Using LIFO, a server-side timeout scheme can\n        reduce the number of connections used during non- peak   periods of\n        use.   When planning for server-side timeouts, ensure that a recycle or\n        pre-ping strategy is in use to gracefully   handle stale connections.\n    \n          .. versionadded:: 1.3\n    \n          .. seealso::\n    \n            :ref:`pool_use_lifo`\n    \n            :ref:`pool_disconnects`\n    \n    :param plugins: string list of plugin names to load.  See\n        :class:`.CreateEnginePlugin` for background.\n    \n        .. versionadded:: 1.2.3\n    \n    :param strategy='plain': selects alternate engine implementations.\n        Currently available are:\n    \n        * the ``threadlocal`` strategy, which is described in\n          :ref:`threadlocal_strategy`;\n        * the ``mock`` strategy, which dispatches all statement\n          execution to a function passed as the argument ``executor``.\n          See `example in the FAQ\n          <http://docs.sqlalchemy.org/en/latest/faq/metadata_schema.html#how-can-i-get-the-create-table-drop-table-output-as-a-string>`_.\n    \n    :param executor=None: a function taking arguments\n        ``(sql, *multiparams, **params)``, to which the ``mock`` strategy will\n        dispatch all statement execution. Used only by ``strategy='mock'``.\n\n"
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    conn.execute("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload json to postgresSQL table\n",
    "# store first level as a column but hours and attributes as jsonb format\n",
    "test = pd.DataFrame(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Help on method to_sql in module pandas.core.generic:\n\nto_sql(name: str, con, schema=None, if_exists: str = 'fail', index: bool = True, index_label=None, chunksize=None, dtype=None, method=None) -> None method of pandas.core.frame.DataFrame instance\n    Write records stored in a DataFrame to a SQL database.\n    \n    Databases supported by SQLAlchemy [1]_ are supported. Tables can be\n    newly created, appended to, or overwritten.\n    \n    Parameters\n    ----------\n    name : str\n        Name of SQL table.\n    con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection\n        Using SQLAlchemy makes it possible to use any DB supported by that\n        library. Legacy support is provided for sqlite3.Connection objects. The user\n        is responsible for engine disposal and connection closure for the SQLAlchemy\n        connectable See `here                 <https://docs.sqlalchemy.org/en/13/core/connections.html>`_.\n    \n    schema : str, optional\n        Specify the schema (if database flavor supports this). If None, use\n        default schema.\n    if_exists : {'fail', 'replace', 'append'}, default 'fail'\n        How to behave if the table already exists.\n    \n        * fail: Raise a ValueError.\n        * replace: Drop the table before inserting new values.\n        * append: Insert new values to the existing table.\n    \n    index : bool, default True\n        Write DataFrame index as a column. Uses `index_label` as the column\n        name in the table.\n    index_label : str or sequence, default None\n        Column label for index column(s). If None is given (default) and\n        `index` is True, then the index names are used.\n        A sequence should be given if the DataFrame uses MultiIndex.\n    chunksize : int, optional\n        Specify the number of rows in each batch to be written at a time.\n        By default, all rows will be written at once.\n    dtype : dict or scalar, optional\n        Specifying the datatype for columns. If a dictionary is used, the\n        keys should be the column names and the values should be the\n        SQLAlchemy types or strings for the sqlite3 legacy mode. If a\n        scalar is provided, it will be applied to all columns.\n    method : {None, 'multi', callable}, optional\n        Controls the SQL insertion clause used:\n    \n        * None : Uses standard SQL ``INSERT`` clause (one per row).\n        * 'multi': Pass multiple values in a single ``INSERT`` clause.\n        * callable with signature ``(pd_table, conn, keys, data_iter)``.\n    \n        Details and a sample callable implementation can be found in the\n        section :ref:`insert method <io.sql.method>`.\n    \n        .. versionadded:: 0.24.0\n    \n    Raises\n    ------\n    ValueError\n        When the table already exists and `if_exists` is 'fail' (the\n        default).\n    \n    See Also\n    --------\n    read_sql : Read a DataFrame from a table.\n    \n    Notes\n    -----\n    Timezone aware datetime columns will be written as\n    ``Timestamp with timezone`` type with SQLAlchemy if supported by the\n    database. Otherwise, the datetimes will be stored as timezone unaware\n    timestamps local to the original timezone.\n    \n    .. versionadded:: 0.24.0\n    \n    References\n    ----------\n    .. [1] https://docs.sqlalchemy.org\n    .. [2] https://www.python.org/dev/peps/pep-0249/\n    \n    Examples\n    --------\n    Create an in-memory SQLite database.\n    \n    >>> from sqlalchemy import create_engine\n    >>> engine = create_engine('sqlite://', echo=False)\n    \n    Create a table from scratch with 3 rows.\n    \n    >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n    >>> df\n         name\n    0  User 1\n    1  User 2\n    2  User 3\n    \n    >>> df.to_sql('users', con=engine)\n    >>> engine.execute(\"SELECT * FROM users\").fetchall()\n    [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n    \n    An `sqlalchemy.engine.Connection` can also be passed to to `con`:\n    >>> with engine.begin() as connection:\n    ...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n    ...     df1.to_sql('users', con=connection, if_exists='append')\n    \n    This is allowed to support operations that require that the same\n    DBAPI connection is used for the entire operation.\n    \n    >>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']})\n    >>> df2.to_sql('users', con=engine, if_exists='append')\n    >>> engine.execute(\"SELECT * FROM users\").fetchall()\n    [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n     (0, 'User 4'), (1, 'User 5'), (0, 'User 6'),\n     (1, 'User 7')]\n    \n    Overwrite the table with just ``df2``.\n    \n    >>> df2.to_sql('users', con=engine, if_exists='replace',\n    ...            index_label='id')\n    >>> engine.execute(\"SELECT * FROM users\").fetchall()\n    [(0, 'User 6'), (1, 'User 7')]\n    \n    Specify the dtype (especially useful for integers with missing values).\n    Notice that while pandas is forced to store the data as floating point,\n    the database supports nullable integers. When fetching the data with\n    Python, we get back integer scalars.\n    \n    >>> df = pd.DataFrame({\"A\": [1, None, 2]})\n    >>> df\n         A\n    0  1.0\n    1  NaN\n    2  2.0\n    \n    >>> from sqlalchemy.types import Integer\n    >>> df.to_sql('integers', con=engine, index=False,\n    ...           dtype={\"A\": Integer()})\n    \n    >>> engine.execute(\"SELECT * FROM integers\").fetchall()\n    [(1,), (None,), (2,)]\n\n"
    }
   ],
   "source": [
    "help(test.to_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no performance differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CREATE TABLE AS (\n",
    "    business_id CHAR(22) PRIMARY KEY \n",
    "    ,address TEXT\n",
    "    ,city TEXT\n",
    "    ,state TEXT\n",
    "    ,postal_code TEXT  \n",
    "    ,latitude FLOAT\n",
    "    ,longitude FLOAT\n",
    "    ,stars FLOAT(1)\n",
    "    ,review_count \n",
    "    ,is_open\n",
    "    ,attributes JSONB\n",
    "    ,categories\n",
    "    ,hours JSONB\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}